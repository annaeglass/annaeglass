## Argument in favor of Chris Anderson

Chris Anderson argues that ‘the data deluge makes the scientific method obsolete.’ Big Data enables an empiricist mode of knowledge production. We can analyze data without hypotheses about what it might reveal. He is right. Thirty years ago, the scientific method required theory and models before data collection, but in an era of abundant data, researchers can surpass the steps of theory and models. Researchers do not need to know what they are looking for, only that there are uses for the massive amounts of data collected. In the era of Big Data, problems solve themselves before researchers know they exist. Anderson argues that we now live in the Petabyte Age, where massive amounts of data, thousands of times bigger than kilobytes, megabytes, terabytes, are all stored in the cloud. The Petabyte Age swaps the order of data collection. 

Now that researchers can obtain massive amounts of data with ease, it becomes inefficient to search for context. Advertising agents have used this phenomenon to create target messages for users of social media platforms. The theorists behind targeted advertising have little idea why someone who bought a lawnmower would then be interested in an awning— only that the purchasing data shows a clear correlation between the two items. Researchers can apply mass amounts of data without knowing the “why,” and Anderson argues that that is the future of Big Data. Anderson argues that mass amounts of data can replace entire fields of study and let the numbers speak for themselves. 

Anderson cites the example of advertising as a major use of Big Data without theory, but he sees larger applications in the scientific fields. Anderson refutes the scientific idea that correlation does not equal causation, instead arguing that massive data transcends the doubt that accompanies the loose correlations that scientists may find after they collect data to test their methods. A primary example, J. Craig Venter implemented shotgun gene sequencing, supercomputers that statistically analyze entire ecosystems to find undiscovered gene sequences. Venter’s gene sequencing found thousands of undiscovered species of bacteria without Venter knowing what he was looking for. Massive data processing has applications in genetics, biology, and neuroscience. Systems, such as the brain, that were previously too complex to tackle are at the fingertips of scientists. 

Anderson argues that a discipline starved of data drifts into theory and past testable hypotheses. Big Data ameliorates the disciplinary drift and allows scientists to delve into more complex underlying realities without losing themselves in the outdated approach of hypothesize, model, test. The more we learn about a field, the further we find ourselves from being able to model possible theories. The scope of knowledge in physics, biology, and others lies far outside of the realm of human thought, and massive amounts of data eliminate the need for humans to develop fallible models that often disprove rather than prove. The scientific method had merit in a realm in which technology limited data collection, but now that scientists can remove that barrier, they must embrace a more efficient and accurate method of drawing conclusions from data. Put frankly, theory is dead because we can let it die.

## Argument in favor of Rob Kitchin

Rob Kitchin addresses the emerging epistemological positions that have accompanied the unfolding data revolution. He argues that there remains a wider critical reflection beyond the “theory is dead” debate. Kitchin presents a bleak side of analyzing data before developing methods. Big Data disrupts mechanisms with which researchers operate and solve problems with data. Kitchin affirms several principles of Big Data that previous theorists have used to explain the unfolding of the Petabyte Age and the death of theory. He maintains, “Big Data is huge in volume, consisting of terabytes or petabytes of data; high in velocity, created in near real-time; diverse in variety, both structured and unstructured; exhaustive in scope, capturing entire populations; fine-grained in resolution and uniquely indexical in identification; relational in nature, containing common fields that can conjoin data sets; flexible, holding the traits of extensionality and scalability.” 

Despite the immense possibilities of Big Data, Kitchin asserts that the complexity of these massive data sets warrants careful analysis before collection. Kitchin cites the example of census data, a large government undertaking, and the inaccuracies present in census data because of its poorly-planned scope. Kitchin takes issue with the difficulties that accompany extracting insight from large amounts of data. Researchers cannot trust that Big Data remains infallible, and Big Data must learn to handle its own abundance and exhaustivity, which brings with it uncertainty and disorder. The field of data science must bring humanity back into the data sets which affect policymakers and individual decisions. Kitchin cites Porway in this plea for human analysis and echos the idea that, without subject matter experts available to articulate issues before mass data, Big Data results in poor or unusable conclusions. Massive data collection becomes insensitive to human needs and instead acts as an inexorable force of analysis that cannot pause to ask critical questions. 

Big Data has merits in the field of data science, as it increases the efficiency of the scientific method. However, eliminating the human aspect entirely leads to a dangerous reality in which scientists fail to ask, ‘why.’ The ‘what’ of data science can identify patterns and has real-world applications, but the ‘why’ of data science furthers progress toward ameliorating human suffering and enhancing freedom. The idea that Big Data transcends human biases or the need for context fails to remember the purpose of data collection— enhancing human freedom. The production of Big Data has far-reaching consequences that will influence the commercial, political, and individual worlds. Although the fourth paradigm of exploratory science yields exciting promises, it makes way for unintended pitfalls. Exploratory science is data-intensive in nature, but the theories that have driven science for thousands of years help to target data collection and pinpoint the conclusions worth finding. It remains useless to collect massive amounts of data if researchers have little or no idea where and why they should apply it.

Kitchin refutes the argument that Big Data enables the empiricist mode of knowledge production. Kitchin acknowledges the merit that this idea of “correlation is enough” in the realm of advertising, but maintains that, in science, this tool becomes dangerous. Big Data often captures a view of the world from a certain vantage point but leaves out other critical viewpoints. Data cannot speak for itself and requires a conceptual framework with which to synthesize patterns in data science and the values of society. Big Data fails to recognize contextualization and the “human” element of science that underpins the implementation of data findings. Correlation is enough leads to false and incomplete conclusions that have harmful implications. 
